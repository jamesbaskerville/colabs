{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPiFsADjRFQze24BddRQmH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesbaskerville/colabs/blob/main/pytorchLightning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSrAYLk_YU7T",
        "outputId": "748eb169-e66b-4f09-a4e8-dc52f6a3612e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.3.3-py3-none-any.whl (808 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.5/808.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.11.5-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.1)\n",
            "Requirement already satisfied: torch<4.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.3.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.3.3-py3-none-any.whl (812 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.3/812.3 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<4.0,>=2.0.0->lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=2.0.0->lightning) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=2.0.0->lightning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=2.0.0->lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2026.0,>=2022.5.0->lightning) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=2.0.0->lightning) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.3.3 lightning-utilities-0.11.5 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.3 torchmetrics-1.4.0.post0\n"
          ]
        }
      ],
      "source": [
        "!pip install lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "import torch\n",
        "\n",
        "print(\"Lightning version:\", L.__version__)\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA is available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "IT325ny7Y_qj",
        "outputId": "0810b247-b63c-436a-adea-3e591b14a898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lightning version: 2.3.3\n",
            "Torch version: 2.3.0+cu121\n",
            "CUDA is available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import lightning as L\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "from lightning.pytorch.loggers.tensorboard import TensorBoardLogger\n",
        "from lightning.pytorch.callbacks.early_stopping import EarlyStopping"
      ],
      "metadata": {
        "id": "Roo6eeG9ZH9B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L.seed_everything(1121218)"
      ],
      "metadata": {
        "id": "y8ewsAu6ZRyv",
        "outputId": "118639c3-0bf9-40e9-b098-7d104bb746af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 1121218\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1121218\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1121218"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "vFyW3WCfZSIa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose(\n",
        "   [\n",
        "       transforms.RandomCrop(32, padding=4),\n",
        "       transforms.RandomHorizontalFlip(),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "   ],\n",
        ")\n",
        "transform_test = transforms.Compose(\n",
        "   [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(\n",
        "   root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "val_dataset = datasets.CIFAR10(\n",
        "   root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")"
      ],
      "metadata": {
        "id": "hQYTQNJAZTWF",
        "outputId": "32c9b7c1-fc51-4c6f-8406-6df21f333255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:10<00:00, 16309790.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "   train_dataset, batch_size=batch_size, shuffle=True, num_workers=8\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "   val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
        ")"
      ],
      "metadata": {
        "id": "-bF3CozjZbxq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a CIFAR-10 classifier with Classic PyTorch\n",
        "class CIFAR10CNN(nn.Module):\n",
        "   def __init__(self):\n",
        "       super(CIFAR10CNN, self).__init__()\n",
        "       # 3 conv layers, max pooling, 2 linears\n",
        "       self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "       self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "       self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "       self.pool = nn.MaxPool2d(2, 2)\n",
        "       self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
        "       self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = self.pool(torch.relu(self.conv1(x)))\n",
        "       x = self.pool(torch.relu(self.conv2(x)))\n",
        "       x = self.pool(torch.relu(self.conv3(x)))\n",
        "       x = x.view(-1, 64 * 4 * 4)\n",
        "       x = torch.relu(self.fc1(x))\n",
        "       x = self.fc2(x)\n",
        "       return x"
      ],
      "metadata": {
        "id": "XZnQibNxZuUh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "kh6MPB6HbdyS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = CIFAR10CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# TensorBoard setup\n",
        "writer = SummaryWriter('runs/cifar10_cnn_experiment')"
      ],
      "metadata": {
        "id": "zeRUoeOtapDs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Calculate average training loss for the epoch\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    writer.add_scalar('training loss', avg_train_loss, epoch)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0.0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "        writer.add_scalar('validation loss', avg_val_loss, epoch)\n",
        "        writer.add_scalar('validation accuracy', accuracy, epoch)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "# Final test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "writer.close()\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'cifar10_cnn.pth')"
      ],
      "metadata": {
        "id": "G1Hy-PikbWVi",
        "outputId": "8bc94e56-4792-4170-93a3-33296f32362a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [100/782], Loss: 2.0440\n",
            "Epoch [1/10], Step [200/782], Loss: 1.5101\n",
            "Epoch [1/10], Step [300/782], Loss: 1.5574\n",
            "Epoch [1/10], Step [400/782], Loss: 1.5169\n",
            "Epoch [1/10], Step [500/782], Loss: 1.3750\n",
            "Epoch [1/10], Step [600/782], Loss: 1.3035\n",
            "Epoch [1/10], Step [700/782], Loss: 1.3609\n",
            "Validation Accuracy: 51.35%\n",
            "Epoch [2/10], Step [100/782], Loss: 0.9979\n",
            "Epoch [2/10], Step [200/782], Loss: 1.3246\n",
            "Epoch [2/10], Step [300/782], Loss: 1.2994\n",
            "Epoch [2/10], Step [400/782], Loss: 1.2299\n",
            "Epoch [2/10], Step [500/782], Loss: 1.1591\n",
            "Epoch [2/10], Step [600/782], Loss: 0.9243\n",
            "Epoch [2/10], Step [700/782], Loss: 1.1823\n",
            "Validation Accuracy: 66.11%\n",
            "Epoch [3/10], Step [100/782], Loss: 0.9658\n",
            "Epoch [3/10], Step [200/782], Loss: 0.9708\n",
            "Epoch [3/10], Step [300/782], Loss: 0.9236\n",
            "Epoch [3/10], Step [400/782], Loss: 1.1136\n",
            "Epoch [3/10], Step [500/782], Loss: 0.8570\n",
            "Epoch [3/10], Step [600/782], Loss: 0.7015\n",
            "Epoch [3/10], Step [700/782], Loss: 0.7606\n",
            "Validation Accuracy: 70.02%\n",
            "Epoch [4/10], Step [100/782], Loss: 1.0419\n",
            "Epoch [4/10], Step [200/782], Loss: 0.6603\n",
            "Epoch [4/10], Step [300/782], Loss: 0.9633\n",
            "Epoch [4/10], Step [400/782], Loss: 0.8254\n",
            "Epoch [4/10], Step [500/782], Loss: 0.8660\n",
            "Epoch [4/10], Step [600/782], Loss: 1.0776\n",
            "Epoch [4/10], Step [700/782], Loss: 0.9477\n",
            "Validation Accuracy: 71.11%\n",
            "Epoch [5/10], Step [100/782], Loss: 0.7097\n",
            "Epoch [5/10], Step [200/782], Loss: 0.9331\n",
            "Epoch [5/10], Step [300/782], Loss: 0.7942\n",
            "Epoch [5/10], Step [400/782], Loss: 0.9605\n",
            "Epoch [5/10], Step [500/782], Loss: 1.0049\n",
            "Epoch [5/10], Step [600/782], Loss: 0.8868\n",
            "Epoch [5/10], Step [700/782], Loss: 0.9281\n",
            "Validation Accuracy: 73.33%\n",
            "Epoch [6/10], Step [100/782], Loss: 0.8912\n",
            "Epoch [6/10], Step [200/782], Loss: 0.7277\n",
            "Epoch [6/10], Step [300/782], Loss: 0.7195\n",
            "Epoch [6/10], Step [400/782], Loss: 0.6245\n",
            "Epoch [6/10], Step [500/782], Loss: 0.8191\n",
            "Epoch [6/10], Step [600/782], Loss: 0.6151\n",
            "Epoch [6/10], Step [700/782], Loss: 0.8012\n",
            "Validation Accuracy: 74.84%\n",
            "Epoch [7/10], Step [100/782], Loss: 0.7479\n",
            "Epoch [7/10], Step [200/782], Loss: 0.6694\n",
            "Epoch [7/10], Step [300/782], Loss: 0.9138\n",
            "Epoch [7/10], Step [400/782], Loss: 0.8623\n",
            "Epoch [7/10], Step [500/782], Loss: 0.5719\n",
            "Epoch [7/10], Step [600/782], Loss: 0.4285\n",
            "Epoch [7/10], Step [700/782], Loss: 0.7525\n",
            "Validation Accuracy: 74.88%\n",
            "Epoch [8/10], Step [100/782], Loss: 0.3738\n",
            "Epoch [8/10], Step [200/782], Loss: 0.6935\n",
            "Epoch [8/10], Step [300/782], Loss: 0.5815\n",
            "Epoch [8/10], Step [400/782], Loss: 0.5247\n",
            "Epoch [8/10], Step [500/782], Loss: 0.5573\n",
            "Epoch [8/10], Step [600/782], Loss: 0.8199\n",
            "Epoch [8/10], Step [700/782], Loss: 0.6678\n",
            "Validation Accuracy: 76.25%\n",
            "Epoch [9/10], Step [100/782], Loss: 0.6303\n",
            "Epoch [9/10], Step [200/782], Loss: 0.8047\n",
            "Epoch [9/10], Step [300/782], Loss: 0.7224\n",
            "Epoch [9/10], Step [400/782], Loss: 0.5394\n",
            "Epoch [9/10], Step [500/782], Loss: 0.6600\n",
            "Epoch [9/10], Step [600/782], Loss: 0.6966\n",
            "Epoch [9/10], Step [700/782], Loss: 0.6567\n",
            "Validation Accuracy: 77.08%\n",
            "Epoch [10/10], Step [100/782], Loss: 0.8521\n",
            "Epoch [10/10], Step [200/782], Loss: 0.4995\n",
            "Epoch [10/10], Step [300/782], Loss: 0.5505\n",
            "Epoch [10/10], Step [400/782], Loss: 0.5915\n",
            "Epoch [10/10], Step [500/782], Loss: 0.6143\n",
            "Epoch [10/10], Step [600/782], Loss: 0.6914\n",
            "Epoch [10/10], Step [700/782], Loss: 0.5560\n",
            "Validation Accuracy: 77.15%\n",
            "Test Accuracy: 77.15%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JWKJjOCVbwaj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}