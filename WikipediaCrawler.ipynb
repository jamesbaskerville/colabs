{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uluRhrocFra1"
      ],
      "authorship_tag": "ABX9TyMR7OLJC+hQ67qNYr504DUw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesbaskerville/colabs/blob/main/WikipediaCrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exploratory project is to crawl English Wikipedia and get a shortest path from source to target, e.g. from [Computer science](https://en.wikipedia.org/wiki/Computer_science) to [Harry R. Lewis](https://en.wikipedia.org/wiki/Harry_R._Lewis).\n",
        "\n",
        "\n",
        "There are two main approaches, utilizing asyncio workers for parallelism:\n",
        "1. BFS using a priority queue ordered by length of path. Basically just doing a naive search using Wikipedia's graph network of links.\n",
        "2. BFS where the priority score is determined by embedding similarity of each article's content to the target article. The idea is that higher similiarity candidates for exploration will better direct our search.\n",
        "\n",
        "In practice, (1) still performs much faster as the API calls for content and model embeddings take significantly longer than each naive BFS operation. However, (2) does find a path while visiting fewer overall pages, giving some credence to the idea that the search is more directed (albeit slower at each node)."
      ],
      "metadata": {
        "id": "b7JZ5CQIfKqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scratchpad"
      ],
      "metadata": {
        "id": "uluRhrocFra1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- will need to build adjacency graph or at least figure out how to queue links\n",
        "- single hop needs how to extract links\n",
        "- this probably looks like beautiful soup for HTML parsing\n",
        "  - simple = find all anchors, filter by ones that start with en.wikipedia.com/wiki\n",
        "- parsers should probably be\n",
        "- where is the content stored? is there a text version of wikipedia\n",
        "\n",
        "#### API\n",
        "https://en.wikipedia.org/w/api.php?action=parse&page=Computer%20science&prop=links&format=json\n",
        "wikipedia has an api that can easily pull links from an article. Maybe good place to start to avoid HTML parsing for now. Just need JSON parsing."
      ],
      "metadata": {
        "id": "gOg2LXtEgJCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "QlL-0Cg2FUkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install aiohttp\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "RRbwNyTm8EiT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QsIarKZne9mG"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import time\n",
        "import logging\n",
        "\n",
        "class RateLimiter:\n",
        "    def __init__(self, requests_per_second):\n",
        "        self.requests_per_second = requests_per_second\n",
        "        self.last_request_time = 0\n",
        "\n",
        "    async def wait_for_rate_limit(self):\n",
        "        current_time = time.time()\n",
        "        time_since_last_request = current_time - self.last_request_time\n",
        "        if time_since_last_request < 1 / self.requests_per_second:\n",
        "            await asyncio.sleep(1 / self.requests_per_second - time_since_last_request)\n",
        "        self.last_request_time = time.time()\n",
        "\n",
        "class WikipediaClient:\n",
        "  def __init__(self, rate_limit=5):\n",
        "    # include rate limiting\n",
        "    self.rate_limiter = RateLimiter(5)\n",
        "\n",
        "  async def fetch_page(self, page: str, prop: str):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "      await self.rate_limiter.wait_for_rate_limit()\n",
        "      if prop == 'content':\n",
        "        url = f'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&explaintext&titles={page}'\n",
        "      else:\n",
        "        url = f'https://en.wikipedia.org/w/api.php?action=parse&page={page}&props=links&format=json'\n",
        "\n",
        "      logging.debug(f\"Fetching {url}\")\n",
        "      async with session.get(url) as response:\n",
        "        response = await response.json()\n",
        "\n",
        "      if prop == 'content':\n",
        "        logging.debug(f\"Fetched content for {url}\")\n",
        "        try:\n",
        "          return '\\n'.join([x['extract'] for x in response['query']['pages'].values()])\n",
        "        except:\n",
        "          print(f\"Error: {response}\")\n",
        "          return \"\"\n",
        "      else:\n",
        "        logging.debug(f\"Fetched links for {url}\")\n",
        "        logging.debug(f\"Response: {response}\")\n",
        "        return [x[\"*\"] for x in filter(lambda x: x['ns'] == 0, response['parse']['links'])]\n",
        "\n",
        "  async def get_adjacent_pages(self, page: str):\n",
        "    page_names = await self.fetch_page(page, 'links')\n",
        "    return page_names\n",
        "\n",
        "  async def get_page_content(self, page: str):\n",
        "    return await self.fetch_page(page, 'content')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import util\n",
        "\n",
        "\n",
        "class SentenceEmbedder:\n",
        "  def __init__(self):\n",
        "    self.model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "  def embed(self, text):\n",
        "    return self.model.encode(text, convert_to_tensor=True)\n",
        "\n",
        "  # 0 to 1 with 0 being most similar\n",
        "  def similarity(self, embedding1, embedding2):\n",
        "    cos_sim = util.cos_sim(embedding1, embedding2).item()\n",
        "    # normalize\n",
        "    return (-1 * cos_sim + 1) / 2\n",
        "\n",
        "test_embedder = SentenceEmbedder()\n",
        "test_embedder.similarity(test_embedder.embed(\"hello how are you\"), test_embedder.embed(\"i really dislike you\"))"
      ],
      "metadata": {
        "id": "zJxWPlf3Vnm1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import random\n",
        "\n",
        "# Naive way to search for target = BFS\n",
        "# Idea = use semantic comparison somehow to word2vec or otherwise\n",
        "# compare current & target and use a priority queue BFS\n",
        "# should also store path and update in in maybe a dictionary instead\n",
        "# of a visited set; e.g. store shortest path to a node in the dict value\n",
        "class WikipediaCrawler:\n",
        "  def __init__(\n",
        "        self,\n",
        "        start_page: str,\n",
        "        target='Kevin Bacon',\n",
        "        max_concurrent=5,\n",
        "        rate_limit=5,\n",
        "        max_depth=5,\n",
        "        strategy=\"depth\",\n",
        "        debug=False\n",
        "    ):\n",
        "    self.start_page = start_page\n",
        "    self.target = target\n",
        "    self.queue = asyncio.PriorityQueue()\n",
        "    self.visited = {}\n",
        "    self.embeddings = {}\n",
        "    self.errored = set()\n",
        "    self.max_concurrent = max_concurrent\n",
        "    self.rate_limit = rate_limit\n",
        "    self.max_depth = max_depth\n",
        "    self.stop_event = asyncio.Event()\n",
        "    self.strategy = strategy\n",
        "    self.embedder = SentenceEmbedder()\n",
        "    self.client = WikipediaClient(rate_limit)\n",
        "    self.workers = []\n",
        "    self.debug = debug\n",
        "\n",
        "  async def priority_score(self, page, prev):\n",
        "    if self.strategy == \"depth\":\n",
        "      return len(self.visited[prev]) if prev in self.visited else 0\n",
        "    elif self.strategy == \"embedding\":\n",
        "      if self.target not in self.embeddings:\n",
        "        target_content = await self.client.get_page_content(self.target)\n",
        "        self.embeddings[self.target] = self.embedder.embed(target_content)\n",
        "\n",
        "      if page not in self.embeddings:\n",
        "        page_content = await self.client.get_page_content(page)\n",
        "        if not page_content:\n",
        "          return 99.0\n",
        "        self.embeddings[page] = self.embedder.embed(page_content)\n",
        "      result = self.embedder.similarity(self.embeddings[page], self.embeddings[self.target])\n",
        "      return result\n",
        "    else:\n",
        "      return 99.0\n",
        "\n",
        "\n",
        "  async def crawl(self):\n",
        "    await self.queue.put((0, self.start_page, None)) # depth/priority, page, prev\n",
        "    self.workers = [asyncio.create_task(self.worker(f'worker{i}')) for i in range(self.max_concurrent)]\n",
        "\n",
        "    try:\n",
        "      await asyncio.gather(*self.workers)\n",
        "      print('Done with workers')\n",
        "      for worker in self.workers:\n",
        "        worker.cancel()\n",
        "    except asyncio.CancelledError:\n",
        "        print(\"Tasks cancelled.\")\n",
        "    finally:\n",
        "        self.stop_event.set()  # Signal workers to stop\n",
        "        # runtime.unassign()  # Terminate Colab runtime\n",
        "\n",
        "    if self.target in self.visited:\n",
        "      print(f\"Found target page {self.target} in {len(self.visited[self.target])} hops\")\n",
        "      print(f\"Path: {self.visited[self.target]}\")\n",
        "      print(f\"Total pages visited: {len(self.visited)}\")\n",
        "      return True\n",
        "    else:\n",
        "      print(f\"Did not find target page {self.target}\")\n",
        "      print(f\"Total pages visited: {len(self.visited)}\")\n",
        "      return False\n",
        "\n",
        "  async def worker(self, workerName):\n",
        "    if self.debug:\n",
        "        print(f\"Starting {workerName}\")\n",
        "\n",
        "    while not self.stop_event.is_set():\n",
        "      prio_score, page, prev = await self.queue.get()\n",
        "      if self.debug:\n",
        "        print(f\"Queue length: {self.queue.qsize()}; processing {page} with score {prio_score}\")\n",
        "\n",
        "      depth = len(self.visited[prev]) if prev else 0\n",
        "      if depth > self.max_depth:\n",
        "        self.queue.task_done()\n",
        "        return\n",
        "\n",
        "      # update to shorter path if possible\n",
        "      if page in self.visited and len(self.visited[prev]) + 1 < len(self.visited[page]):\n",
        "        self.visited[page] = self.visited[prev] + [prev]\n",
        "        self.queue.task_done()\n",
        "        continue\n",
        "\n",
        "      if page not in self.visited:\n",
        "        if not prev:\n",
        "          self.visited[page] = [page]\n",
        "        else:\n",
        "          self.visited[page] = self.visited[prev] + [page]\n",
        "\n",
        "        if self.strategy == 'embedding' or len(self.visited) % 200 == 0:\n",
        "          print(f\"Processed {len(self.visited)} pages, currently at depth {depth}, queue length {self.queue.qsize()}\")\n",
        "\n",
        "        try:\n",
        "          adjacent_pages = await self.client.get_adjacent_pages(page)\n",
        "\n",
        "          if self.debug:\n",
        "            print(f\"Found {len(adjacent_pages)} adjacent pages for {page}\")\n",
        "\n",
        "          # for the embeddings approach iterating through everything is quite heavy\n",
        "          # maybe should just sample some and move on\n",
        "          for adjacent_page in adjacent_pages:\n",
        "            if self.stop_event.is_set():\n",
        "              if self.debug:\n",
        "                print(f\"Stopping {workerName}\")\n",
        "              self.queue.task_done()\n",
        "              return\n",
        "\n",
        "            # Success!\n",
        "            if adjacent_page == self.target:\n",
        "              self.visited[self.target] = self.visited[prev] + [page, adjacent_page]\n",
        "              self.queue.task_done()\n",
        "              print(f\"Found target with path: {self.visited[self.target]}\")\n",
        "              self.stop_event.set()\n",
        "\n",
        "            elif adjacent_page not in self.visited and depth + 1 <= self.max_depth:\n",
        "              new_prio_score = await self.priority_score(adjacent_page, prev)\n",
        "              if self.debug:\n",
        "                print(f\"Adding {adjacent_page} with priority {new_prio_score}\")\n",
        "              await self.queue.put((new_prio_score, adjacent_page, page))\n",
        "            else:\n",
        "              pass\n",
        "        except Exception as e:\n",
        "          if self.debug:\n",
        "            print(f\"Error processing {page}: {e}\")\n",
        "            traceback.print_exc()\n",
        "          self.errored.add(page)\n",
        "          # retry once\n",
        "          if page not in self.errored:\n",
        "            if self.debug:\n",
        "              print(f\"Retrying {page}\")\n",
        "            await self.queue.put((prio_score, page, prev))\n",
        "\n",
        "      self.queue.task_done()\n",
        "    print(f\"{workerName} exiting gracefully.\")\n",
        "    return"
      ],
      "metadata": {
        "id": "qFDF6swyVn9S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use BFS alone -- order priority queue by length of path\n",
        "crawler = WikipediaCrawler(\n",
        "    'Computer science',\n",
        "    target=\"Singapore\",\n",
        "    rate_limit=5,\n",
        "    max_depth=10,\n",
        "    strategy=\"depth\"\n",
        "  )\n",
        "await crawler.crawl()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xh4hzsZa8Mjw",
        "outputId": "2bc45029-205c-495c-fcc2-62fd02b5155c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 5 pages, currently at depth 1, queue length 556\n",
            "Processed 10 pages, currently at depth 1, queue length 1687\n",
            "Processed 15 pages, currently at depth 1, queue length 3897\n",
            "Processed 20 pages, currently at depth 1, queue length 5010\n",
            "Processed 25 pages, currently at depth 1, queue length 6506\n",
            "Processed 30 pages, currently at depth 1, queue length 7089\n",
            "Found target with path: ['Computer science', 'Artificial intelligence', 'Singapore']\n",
            "Done with workers\n",
            "Found target page Singapore in 3 hops\n",
            "Path: ['Computer science', 'Artificial intelligence', 'Singapore']\n",
            "Total pages visited: 34\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use text embeddings of article content to guide search\n",
        "crawler = WikipediaCrawler(\n",
        "    'Computer science',\n",
        "    target=\"Singapore\",\n",
        "    rate_limit=5,\n",
        "    max_depth=2,\n",
        "    strategy=\"embedding\"\n",
        "  )\n",
        "await crawler.crawl()"
      ],
      "metadata": {
        "id": "XTmXXVmmdRVb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7875aa0c-d1a3-47be-c90e-46593395ba17"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1 pages, currently at depth 0, queue length 0\n",
            "Processed 2 pages, currently at depth 1, queue length 0\n",
            "Processed 3 pages, currently at depth 1, queue length 0\n",
            "Processed 4 pages, currently at depth 1, queue length 0\n",
            "Processed 5 pages, currently at depth 1, queue length 0\n",
            "Processed 6 pages, currently at depth 1, queue length 3\n",
            "Processed 7 pages, currently at depth 2, queue length 7\n",
            "Processed 8 pages, currently at depth 2, queue length 13\n",
            "Processed 9 pages, currently at depth 2, queue length 19\n",
            "Processed 10 pages, currently at depth 1, queue length 26\n",
            "Error: {'batchcomplete': '', 'query': {'pages': {'-1': {'ns': 0, 'title': 'List of important publications in theoretical computer science', 'missing': ''}}}}\n",
            "Processed 11 pages, currently at depth 2, queue length 1369\n",
            "Processed 12 pages, currently at depth 2, queue length 1373\n",
            "Found target with path: ['Computer science', 'History of computing', 'China', 'Singapore']\n",
            "Done with workers\n",
            "Found target page Singapore in 4 hops\n",
            "Path: ['Computer science', 'History of computing', 'China', 'Singapore']\n",
            "Total pages visited: 13\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}